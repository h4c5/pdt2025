{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atelier tokenizer\n",
    "\n",
    "Dans cet atelier nous alons constuire un tokenizer utilisant l'algorithme Byte Pair Encoding (BPE) tel que ceux utilis√©s\n",
    "dans ChatGPT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir une chaine de caract√®res en une s√©quence d'entiers\n",
    "\n",
    "D'apr√®s la [documentation python](https://docs.python.org/fr/3/library/stdtypes.html#text-sequence-type-str) :\n",
    "\n",
    "> Les cha√Ænes sont des s√©quences immuables de points de code Unicode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque caract√®re a un num√©ro, une cat√©gorie, un nom :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B 66 Lu LATIN CAPITAL LETTER B\n",
      "o 111 Ll LATIN SMALL LETTER O\n",
      "n 110 Ll LATIN SMALL LETTER N\n",
      "j 106 Ll LATIN SMALL LETTER J\n",
      "o 111 Ll LATIN SMALL LETTER O\n",
      "u 117 Ll LATIN SMALL LETTER U\n",
      "r 114 Ll LATIN SMALL LETTER R\n",
      "  32 Zs SPACE\n",
      "üëã 128075 So WAVING HAND SIGN\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "text = \"Bonjour üëã\"\n",
    "\n",
    "for char in text:\n",
    "    print(char, ord(char), unicodedata.category(char), unicodedata.name(char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les chaines de caract√®res sont ensuite encod√©es pour permettre la sauvegarde, la lecture, etc.\n",
    "\n",
    "Il existe plusieurs types d'encodage. Le plus utilis√© est `UTF-8`. Cet encodage repr√©sente chaque point de code par une suite de 1 √† 4 bytes en fonction de son indice :\n",
    "\n",
    "\n",
    "| Premier point de code | Dernier code point | Byte 1       | Byte 2   | Byte 3   | Byte 4   |\n",
    "|-----------------------|--------------------|--------------|----------|----------|----------|\n",
    "| U+0000 (0)            | U+007F (127)       | **0**yyyzzzz |          |          |          |\n",
    "| U+0080 (128)          | U+07FF (2047)      | **110**xxxyy | 10yyzzzz |          |          |\n",
    "| U+0800 (2048)         | U+FFFF (65535)     | **1110**wwww | 10xxxxyy | 10yyzzzz |          |\n",
    "| U+010000 (65536)      | U+10FFFF (1114111) | **11110**uvv | 10vvwwww | 10xxxxyy | 10yyzzzz |\n",
    "\n",
    "<br/>\n",
    "\n",
    "> **Rappel :** Un _byte_ en anglais correspond √† un _octet_ en fran√ßais soit 8 _bits_.\n",
    "> \n",
    "> Un octet peut donc prendre 2^8 = 256 valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B | 66 | code hexad√©cimal : 42\n",
      "o | 111 | code hexad√©cimal : 6f\n",
      "n | 110 | code hexad√©cimal : 6e\n",
      "j | 106 | code hexad√©cimal : 6a\n",
      "o | 111 | code hexad√©cimal : 6f\n",
      "u | 117 | code hexad√©cimal : 75\n",
      "r | 114 | code hexad√©cimal : 72\n",
      "  | 32 | code hexad√©cimal : 20\n",
      "üëã | 240 | code hexad√©cimal : f0\n",
      "üëã | 159 | code hexad√©cimal : 9f\n",
      "üëã | 145 | code hexad√©cimal : 91\n",
      "üëã | 139 | code hexad√©cimal : 8b\n"
     ]
    }
   ],
   "source": [
    "text = \"Bonjour üëã\"\n",
    "\n",
    "for char in text:\n",
    "    for byte in char.encode(\"utf-8\"):\n",
    "        print(char, \"|\", byte, f\"| code hexad√©cimal : {byte:02x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut directement obtenir la liste des bytes, il s'agit d'une premi√®re tokenization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 111, 110, 106, 111, 117, 114, 32, 240, 159, 145, 139]\n"
     ]
    }
   ],
   "source": [
    "text = \"Bonjour üëã\"\n",
    "text_ids = list(text.encode())\n",
    "\n",
    "print(text_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impl√©mentation de la tokenization Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 111, 110, 106, 111, 117, 114, 32, 240, 159, 145, 139]\n"
     ]
    }
   ],
   "source": [
    "text = \"Bonjour üëã\"\n",
    "text_ids = list(text.encode())\n",
    "print(text_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction permettra de tester les diff√©rentes fonctions de l'atelier\n",
    "from tests import test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[L'algorithme BPE ](https://en.wikipedia.org/wiki/Byte_pair_encoding) fonctionne en fusionnant sucessivement la paire\n",
    "de tokens la plus fr√©quente.\n",
    "\n",
    "Dans un premier temps, on va donc impl√©menter une fonction qui d√©termine la paire de tokens la plus fr√©quente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, 110, 101, 32, 100, 101, 109, 105, 45, 104, 101, 117, 114, 101, 32, 112, 108, 117, 115, 32, 116, 97, 114, 100, 44, 32, 72, 97, 114, 114, 121, 44, 32, 113, 117, 105, 32, 110, 39, 101, 110, 32, 99, 114, 111, 121, 97, 105, 116, 32, 112, 97, 115, 32, 115, 97, 32, 99, 104, 97, 110, 99, 101, 44, 32, 195, 169, 116, 97, 105, 116, 32, 97, 115, 115, 105, 115, 32, 195, 160, 32, 108, 39, 97, 114, 114, 105, 195, 168, 114, 101, 32, 100, 101, 32, 108, 97, 32, 118, 111, 105, 116, 117, 114, 101, 32, 100, 101, 115, 10, 68, 117, 114, 115, 108, 101, 121, 44, 32, 101, 110, 32, 99, 111, 109, 112, 97, 103, 110, 105, 101, 32, 100, 101, 32, 80, 105, 101, 114, 115, 32, 101, 116, 32, 68, 117, 100, 108, 101, 121, 46, 32, 80, 111, 117, 114, 32, 108, 97, 32, 112, 114, 101, 109, 105, 195, 168, 114, 101, 32, 102, 111, 105, 115, 32, 100, 101, 32, 115, 97, 32, 118, 105, 101, 44, 32, 105, 108, 32, 97, 108, 108, 97, 105, 116, 32, 118, 105, 115, 105, 116, 101, 114, 32, 108, 101, 32, 122, 111, 111, 46]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Une demi-heure plus tard, Harry, qui n'en croyait pas sa chance, √©tait assis √† l'arri√®re de la voiture des\n",
    "Dursley, en compagnie de Piers et Dudley. Pour la premi√®re fois de sa vie, il allait visiter le zoo.\"\"\"\n",
    "\n",
    "text_ids = list(text.encode())\n",
    "print(text_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_pair(ids: list[int]) -> tuple[tuple[int, int], int] | None:\n",
    "    \"\"\"R√©cup√©ration de la paire de tokens la plus fr√©quente.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2), 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_pair([1, 2, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TEST 0\n",
      "‚úÖ TEST 1\n",
      "‚úÖ TEST 2\n"
     ]
    }
   ],
   "source": [
    "test(get_top_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paire la plus fr√©quente : (101, 32) = ('e ') avec 10 apparitions\n"
     ]
    }
   ],
   "source": [
    "assert (top_pair := get_top_pair(text_ids))\n",
    "\n",
    "pair, nb = top_pair\n",
    "pair_str = bytes(pair).decode(\"utf-8\")\n",
    "\n",
    "print(f\"Paire la plus fr√©quente : {pair} = ('{pair_str}') avec {nb} apparitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on va √©cfire une fonction qui fusionne une paire en rempla√ßant les octets par une nouvel id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 1, 4, 3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge(ids: list[int], pair: tuple[int, int], new_id: int) -> list[int]:\n",
    "    \"\"\"Fusion d'une pair de tokens.\"\"\"\n",
    "\n",
    "\n",
    "merge([1, 2, 3, 1, 2, 3, 3], (2, 3), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TEST 0\n",
      "‚úÖ TEST 1\n",
      "‚úÖ TEST 2\n",
      "‚úÖ TEST 3\n"
     ]
    }
   ],
   "source": [
    "test(merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du tokenizer\n",
    "\n",
    "Les fonctions `get_top_pair` et `merge` peuvent maintenant √™tre utilis√©e pour entrainer un tokenizer BPE.\n",
    "\n",
    "Ecrivons une fonction `train_bpe_tokenizer` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens initial : 211\n",
      "Nombre de tokens final   : 165\n",
      "Ratio de compression     : 78%\n"
     ]
    }
   ],
   "source": [
    "def train_bpe_tokenizer(text: str, vocab_size: int) -> dict[tuple[int, int], int]:\n",
    "    \"\"\"Entrainement d'un tokenizer BPE.\"\"\"\n",
    "\n",
    "\n",
    "merges = train_bpe_tokenizer(text, 266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens initial : 26\n",
      "Nombre de tokens final   : 9\n",
      "Ratio de compression     : 35%\n",
      "‚úÖ TEST 0\n",
      "Nombre de tokens initial : 0\n",
      "Nombre de tokens final   : 0\n",
      "Ratio de compression     : 0%\n",
      "‚úÖ TEST 1\n",
      "‚úÖ TEST 2\n"
     ]
    }
   ],
   "source": [
    "test(train_bpe_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodage (tokenisation)\n",
    "\n",
    "A pr√©sent on peut tokenizer une chaine de caract√®re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "[85, 110, 260, 101, 109, 105, 45, 104, 101, 261, 256, 112, 108, 117, 257, 116, 263, 100, 258, 72, 263, 114, 121, 258, 113, 117, 105, 32, 110, 39, 101, 110, 32, 99, 114, 111, 121, 265, 112, 97, 257, 115, 262, 99, 104, 97, 110, 99, 101, 258, 195, 169, 116, 265, 97, 115, 115, 105, 257, 195, 160, 32, 108, 39, 263, 114, 105, 195, 168, 114, 260, 256, 108, 262, 118, 111, 259, 261, 260, 101, 115, 10, 68, 261, 115, 108, 101, 121, 258, 101, 110, 32, 99, 111, 109, 112, 97, 103, 110, 105, 260, 256, 80, 105, 101, 114, 257, 101, 116, 32, 68, 117, 100, 108, 101, 121, 46, 32, 80, 111, 261, 32, 108, 262, 112, 114, 101, 109, 105, 195, 168, 114, 256, 102, 111, 105, 257, 100, 256, 115, 262, 118, 105, 101, 258, 105, 108, 32, 97, 108, 108, 265, 118, 105, 115, 259, 101, 114, 32, 108, 256, 122, 111, 111, 46]\n"
     ]
    }
   ],
   "source": [
    "def encode(text: str, merges: dict[tuple[int, int], int]) -> list[int]:\n",
    "    \"\"\"Tokenization d'une chaine de caract√®re √† partir du dictionnaire merges.\"\"\"\n",
    "\n",
    "\n",
    "ids = encode(text, merges)\n",
    "print(len(ids))\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TEST 0\n",
      "‚úÖ TEST 1\n",
      "‚úÖ TEST 2\n"
     ]
    }
   ],
   "source": [
    "test(encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D√©codage\n",
    "\n",
    "Maintenant on souhaite d√©coder une s√©quence de tokens.\n",
    "\n",
    "Dans un premier temps, on va construire un vocabulaire qui √† chaque token, va associ√© les octets correspondant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'bo', 257: b'bon', 258: b'bonj', 259: b'bonjo', 260: b'bonjou', 261: b'bonjour'}\n"
     ]
    }
   ],
   "source": [
    "def construct_vocab(merges: dict[tuple[int, int], int]) -> dict[int, bytes]:\n",
    "    \"\"\"Construction d'un vocabulaire √† partir d'un dictionnaire merges.\"\"\"\n",
    "\n",
    "\n",
    "vocab = construct_vocab(merges)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TEST 0\n",
      "‚úÖ TEST 1\n"
     ]
    }
   ],
   "source": [
    "test(construct_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Une demi-heure plus tard, Harry, qui n'en croyait pas sa chance, √©tait assis √† l'arri√®re de la voiture des\n",
      "Dursley, en compagnie de Piers et Dudley. Pour la premi√®re fois de sa vie, il allait visiter le zoo.\n"
     ]
    }
   ],
   "source": [
    "def decode(tokens: list[int], vocab: dict[int, bytes]) -> str:\n",
    "    \"\"\"D√©codage d'une s√©quence de tokens\"\"\"\n",
    "\n",
    "\n",
    "print(decode(ids, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TEST 0\n",
      "‚úÖ TEST 1\n",
      "‚úÖ TEST 2\n",
      "‚úÖ TEST 3\n"
     ]
    }
   ],
   "source": [
    "test(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le texte doit √™tre pr√©serv√© lorsqu'on l'encode puis le d√©code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"Bonjour üëã\"\n",
    "assert t == decode(encode(t, merges), vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : Tokenization BPE et regex !\n",
    "\n",
    "Une des limites de l'algorithme BPE est que les mots courants tels que `chien` peuvent tout √† faire se retrouver dans\n",
    "plusieurs tokens avec une ponctuation diff√©rente :  `chien`, ` chien`,`chien.`, `chien,`, `chien!`, `chien?`.\n",
    "\n",
    "Cela est probl√©matique pour les mod√®les de langage car le mod√®le doit \"comprendre\" √† partir des textes sur lesquels il\n",
    "est entraint√© que tous ces tokens d√©signe le m√™me concept.\n",
    "\n",
    "C'est √©galement un probl√®me pour les nombres car il peut tr√®s bien y avoir uniquement des tokens `1`, `11`, `2` ce qui\n",
    "provoque des tokenization √©trange des nombres : \n",
    "- `123` -> `1`, `2`, `3`\n",
    "- `112` -> `11`, `2`\n",
    "\n",
    "Pour √©viter ce genre de ph√©nom√®ne, OpenAI utilise en r√©alit√© une variante de l'algorithme BPE qui d√©coupe au pr√©alable\n",
    "le texte √† l'aide d'une regex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(r\"\"\" ?[^\\W\\d_]+| ?\\d{1,3}|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "\n",
    "pattern.findall(\n",
    "    \"üëã ‰Ω†Â•Ω   Bonjour _xXxFortiche123. Vous avez : 123456 ‚Ç¨ sur votre compte.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI utilise des regex plus compliqu√©s pour ces tokenizers :\n",
    "\n",
    "```python\n",
    "# the main GPT text split patterns, see\n",
    "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut r√©√©crire les fonctions `count_pairs` et `train_bpe_tokenizer` pour les faire fonctionner sur des s√©quences de tokens :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_pair_sequences(tokens_sequences: list[list[int]]):\n",
    "    \"\"\"R√©cup√®re la paire la plus fr√©quente parmi une liste de liste de tokens.\"\"\"\n",
    "\n",
    "\n",
    "def train_bpe_tokenizer_sequences(\n",
    "    text: str, vocab_size: int, pattern: re.Pattern = pattern\n",
    "):\n",
    "    \"\"\"Entrainement d'un tokenizer BPE avec une regex.\"\"\"\n",
    "\n",
    "\n",
    "merges = train_bpe_tokenizer_sequences(text, 266)\n",
    "merges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
