{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atelier tokenizer\n",
    "\n",
    "Dans cet atelier nous alons constuire un tokenizer utilisant l'algorithme Byte Pair Encoding (BPE) tel que ceux utilisÃ©s\n",
    "dans ChatGPT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir une chaine de caractÃ¨res en une sÃ©quence d'entiers\n",
    "\n",
    "D'aprÃ¨s la [documentation python](https://docs.python.org/fr/3/library/stdtypes.html#text-sequence-type-str) :\n",
    "\n",
    "> Les chaÃ®nes sont des sÃ©quences immuables de points de code Unicode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque caractÃ¨re a un numÃ©ro, une catÃ©gorie, un nom :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B 66 Lu LATIN CAPITAL LETTER B\n",
      "o 111 Ll LATIN SMALL LETTER O\n",
      "n 110 Ll LATIN SMALL LETTER N\n",
      "j 106 Ll LATIN SMALL LETTER J\n",
      "o 111 Ll LATIN SMALL LETTER O\n",
      "u 117 Ll LATIN SMALL LETTER U\n",
      "r 114 Ll LATIN SMALL LETTER R\n",
      "  32 Zs SPACE\n",
      "ðŸ‘‹ 128075 So WAVING HAND SIGN\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "text = \"Bonjour ðŸ‘‹\"\n",
    "\n",
    "for char in text:\n",
    "    print(char, ord(char), unicodedata.category(char), unicodedata.name(char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les chaines de caractÃ¨res sont ensuite encodÃ©es pour permettre la sauvegarde, la lecture, etc.\n",
    "\n",
    "Il existe plusieurs types d'encodage. Le plus utilisÃ© est `UTF-8`. Cet encodage reprÃ©sente chaque point de code par une suite de 1 Ã  4 bytes en fonction de son indice :\n",
    "\n",
    "\n",
    "| Premier point de code | Dernier code point | Byte 1       | Byte 2   | Byte 3   | Byte 4   |\n",
    "|-----------------------|--------------------|--------------|----------|----------|----------|\n",
    "| U+0000 (0)            | U+007F (127)       | **0**yyyzzzz |          |          |          |\n",
    "| U+0080 (128)          | U+07FF (2047)      | **110**xxxyy | 10yyzzzz |          |          |\n",
    "| U+0800 (2048)         | U+FFFF (65535)     | **1110**wwww | 10xxxxyy | 10yyzzzz |          |\n",
    "| U+010000 (65536)      | U+10FFFF (1114111) | **11110**uvv | 10vvwwww | 10xxxxyy | 10yyzzzz |\n",
    "\n",
    "<br/>\n",
    "\n",
    "> **Rappel :** Un _byte_ en anglais correspond Ã  un _octet_ en franÃ§ais soit 8 _bits_.\n",
    "> \n",
    "> Un octet peut donc prendre 2^8 = 256 valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B | 66 | code hexadÃ©cimal : 42\n",
      "o | 111 | code hexadÃ©cimal : 6f\n",
      "n | 110 | code hexadÃ©cimal : 6e\n",
      "j | 106 | code hexadÃ©cimal : 6a\n",
      "o | 111 | code hexadÃ©cimal : 6f\n",
      "u | 117 | code hexadÃ©cimal : 75\n",
      "r | 114 | code hexadÃ©cimal : 72\n",
      "  | 32 | code hexadÃ©cimal : 20\n",
      "ðŸ‘‹ | 240 | code hexadÃ©cimal : f0\n",
      "ðŸ‘‹ | 159 | code hexadÃ©cimal : 9f\n",
      "ðŸ‘‹ | 145 | code hexadÃ©cimal : 91\n",
      "ðŸ‘‹ | 139 | code hexadÃ©cimal : 8b\n"
     ]
    }
   ],
   "source": [
    "text = \"Bonjour ðŸ‘‹\"\n",
    "\n",
    "for char in text:\n",
    "    for byte in char.encode(\"utf-8\"):\n",
    "        print(char, \"|\", byte, f\"| code hexadÃ©cimal : {byte:02x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut directement obtenir la liste des bytes, il s'agit d'une premiÃ¨re tokenization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 111, 110, 106, 111, 117, 114, 32, 240, 159, 145, 139]\n"
     ]
    }
   ],
   "source": [
    "text = \"Bonjour ðŸ‘‹\"\n",
    "text_ids = list(text.encode())\n",
    "\n",
    "print(text_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImplÃ©mentation de la tokenization Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 111, 110, 106, 111, 117, 114, 32, 240, 159, 145, 139]\n"
     ]
    }
   ],
   "source": [
    "text = \"Bonjour ðŸ‘‹\"\n",
    "text_ids = list(text.encode())\n",
    "print(text_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction permettra de tester les diffÃ©rentes fonctions de l'atelier\n",
    "from tests import test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[L'algorithme BPE ](https://en.wikipedia.org/wiki/Byte_pair_encoding) fonctionne en fusionnant sucessivement la paire\n",
    "de tokens la plus frÃ©quente.\n",
    "\n",
    "Dans un premier temps, on va donc implÃ©menter une fonction qui dÃ©termine la paire de tokens la plus frÃ©quente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, 110, 101, 32, 100, 101, 109, 105, 45, 104, 101, 117, 114, 101, 32, 112, 108, 117, 115, 32, 116, 97, 114, 100, 44, 32, 72, 97, 114, 114, 121, 44, 32, 113, 117, 105, 32, 110, 39, 101, 110, 32, 99, 114, 111, 121, 97, 105, 116, 32, 112, 97, 115, 32, 115, 97, 32, 99, 104, 97, 110, 99, 101, 44, 32, 195, 169, 116, 97, 105, 116, 32, 97, 115, 115, 105, 115, 32, 195, 160, 32, 108, 39, 97, 114, 114, 105, 195, 168, 114, 101, 32, 100, 101, 32, 108, 97, 32, 118, 111, 105, 116, 117, 114, 101, 32, 100, 101, 115, 10, 68, 117, 114, 115, 108, 101, 121, 44, 32, 101, 110, 32, 99, 111, 109, 112, 97, 103, 110, 105, 101, 32, 100, 101, 32, 80, 105, 101, 114, 115, 32, 101, 116, 32, 68, 117, 100, 108, 101, 121, 46, 32, 80, 111, 117, 114, 32, 108, 97, 32, 112, 114, 101, 109, 105, 195, 168, 114, 101, 32, 102, 111, 105, 115, 32, 100, 101, 32, 115, 97, 32, 118, 105, 101, 44, 32, 105, 108, 32, 97, 108, 108, 97, 105, 116, 32, 118, 105, 115, 105, 116, 101, 114, 32, 108, 101, 32, 122, 111, 111, 46]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Une demi-heure plus tard, Harry, qui n'en croyait pas sa chance, Ã©tait assis Ã  l'arriÃ¨re de la voiture des\n",
    "Dursley, en compagnie de Piers et Dudley. Pour la premiÃ¨re fois de sa vie, il allait visiter le zoo.\"\"\"\n",
    "\n",
    "text_ids = list(text.encode())\n",
    "print(text_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_pair(ids: list[int]) -> tuple[tuple[int, int], int] | None:\n",
    "    \"\"\"RÃ©cupÃ©ration de la paire de tokens la plus frÃ©quente.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2), 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_pair([1, 2, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TEST 0\n",
      "âœ… TEST 1\n",
      "âœ… TEST 2\n"
     ]
    }
   ],
   "source": [
    "test(get_top_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paire la plus frÃ©quente : (101, 32) = ('e ') avec 10 apparitions\n"
     ]
    }
   ],
   "source": [
    "assert (top_pair := get_top_pair(text_ids))\n",
    "\n",
    "pair, nb = top_pair\n",
    "pair_str = bytes(pair).decode(\"utf-8\")\n",
    "\n",
    "print(f\"Paire la plus frÃ©quente : {pair} = ('{pair_str}') avec {nb} apparitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on va Ã©cfire une fonction qui fusionne une paire en remplaÃ§ant les octets par une nouvel id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 1, 4, 3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge(ids: list[int], pair: tuple[int, int], new_id: int) -> list[int]:\n",
    "    \"\"\"Fusion d'une pair de tokens.\"\"\"\n",
    "\n",
    "\n",
    "merge([1, 2, 3, 1, 2, 3, 3], (2, 3), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TEST 0\n",
      "âœ… TEST 1\n",
      "âœ… TEST 2\n",
      "âœ… TEST 3\n"
     ]
    }
   ],
   "source": [
    "test(merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du tokenizer\n",
    "\n",
    "Les fonctions `get_top_pair` et `merge` peuvent maintenant Ãªtre utilisÃ©e pour entrainer un tokenizer BPE.\n",
    "\n",
    "Ecrivons une fonction `train_bpe_tokenizer` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens initial : 211\n",
      "Nombre de tokens final   : 165\n",
      "Ratio de compression     : 78%\n"
     ]
    }
   ],
   "source": [
    "def train_bpe_tokenizer(text: str, vocab_size: int) -> dict[tuple[int, int], int]:\n",
    "    \"\"\"Entrainement d'un tokenizer BPE.\"\"\"\n",
    "\n",
    "\n",
    "merges = train_bpe_tokenizer(text, 266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens initial : 26\n",
      "Nombre de tokens final   : 9\n",
      "Ratio de compression     : 35%\n",
      "âœ… TEST 0\n",
      "Nombre de tokens initial : 0\n",
      "Nombre de tokens final   : 0\n",
      "Ratio de compression     : 0%\n",
      "âœ… TEST 1\n",
      "âœ… TEST 2\n"
     ]
    }
   ],
   "source": [
    "test(train_bpe_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodage (tokenisation)\n",
    "\n",
    "A prÃ©sent on peut tokenizer une chaine de caractÃ¨re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "[85, 110, 260, 101, 109, 105, 45, 104, 101, 261, 256, 112, 108, 117, 257, 116, 263, 100, 258, 72, 263, 114, 121, 258, 113, 117, 105, 32, 110, 39, 101, 110, 32, 99, 114, 111, 121, 265, 112, 97, 257, 115, 262, 99, 104, 97, 110, 99, 101, 258, 195, 169, 116, 265, 97, 115, 115, 105, 257, 195, 160, 32, 108, 39, 263, 114, 105, 195, 168, 114, 260, 256, 108, 262, 118, 111, 259, 261, 260, 101, 115, 10, 68, 261, 115, 108, 101, 121, 258, 101, 110, 32, 99, 111, 109, 112, 97, 103, 110, 105, 260, 256, 80, 105, 101, 114, 257, 101, 116, 32, 68, 117, 100, 108, 101, 121, 46, 32, 80, 111, 261, 32, 108, 262, 112, 114, 101, 109, 105, 195, 168, 114, 256, 102, 111, 105, 257, 100, 256, 115, 262, 118, 105, 101, 258, 105, 108, 32, 97, 108, 108, 265, 118, 105, 115, 259, 101, 114, 32, 108, 256, 122, 111, 111, 46]\n"
     ]
    }
   ],
   "source": [
    "def encode(text: str, merges: dict[tuple[int, int], int]) -> list[int]:\n",
    "    \"\"\"Tokenization d'une chaine de caractÃ¨re Ã  partir du dictionnaire merges.\"\"\"\n",
    "\n",
    "\n",
    "ids = encode(text, merges)\n",
    "print(len(ids))\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TEST 0\n",
      "âœ… TEST 1\n",
      "âœ… TEST 2\n"
     ]
    }
   ],
   "source": [
    "test(encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DÃ©codage\n",
    "\n",
    "Maintenant on souhaite dÃ©coder une sÃ©quence de tokens.\n",
    "\n",
    "Dans un premier temps, on va construire un vocabulaire qui Ã  chaque token, va associÃ© les octets correspondant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'bo', 257: b'bon', 258: b'bonj', 259: b'bonjo', 260: b'bonjou', 261: b'bonjour'}\n"
     ]
    }
   ],
   "source": [
    "def construct_vocab(merges: dict[tuple[int, int], int]) -> dict[int, bytes]:\n",
    "    \"\"\"Construction d'un vocabulaire Ã  partir d'un dictionnaire merges.\"\"\"\n",
    "\n",
    "\n",
    "vocab = construct_vocab(merges)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TEST 0\n",
      "âœ… TEST 1\n"
     ]
    }
   ],
   "source": [
    "test(construct_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Une demi-heure plus tard, Harry, qui n'en croyait pas sa chance, Ã©tait assis Ã  l'arriÃ¨re de la voiture des\n",
      "Dursley, en compagnie de Piers et Dudley. Pour la premiÃ¨re fois de sa vie, il allait visiter le zoo.\n"
     ]
    }
   ],
   "source": [
    "def decode(tokens: list[int], vocab: dict[int, bytes]) -> str:\n",
    "    \"\"\"DÃ©codage d'une sÃ©quence de tokens\"\"\"\n",
    "\n",
    "\n",
    "print(decode(ids, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TEST 0\n",
      "âœ… TEST 1\n",
      "âœ… TEST 2\n",
      "âœ… TEST 3\n"
     ]
    }
   ],
   "source": [
    "test(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le texte doit Ãªtre prÃ©servÃ© lorsqu'on l'encode puis le dÃ©code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"Bonjour ðŸ‘‹\"\n",
    "assert t == decode(encode(t, merges), vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : Tokenization BPE et regex !\n",
    "\n",
    "Une des limites de l'algorithme BPE est que les mots courants tels que `chien` peuvent tout Ã  faire se retrouver dans\n",
    "plusieurs tokens avec une ponctuation diffÃ©rente :  `chien`, ` chien`,`chien.`, `chien,`, `chien!`, `chien?`.\n",
    "\n",
    "Cela est problÃ©matique pour les modÃ¨les de langage car le modÃ¨le doit \"comprendre\" Ã  partir des textes sur lesquels il\n",
    "est entraintÃ© que tous ces tokens dÃ©signe le mÃªme concept.\n",
    "\n",
    "C'est Ã©galement un problÃ¨me pour les nombres car il peut trÃ¨s bien y avoir uniquement des tokens `1`, `11`, `2` ce qui\n",
    "provoque des tokenization Ã©trange des nombres : \n",
    "- `123` -> `1`, `2`, `3`\n",
    "- `112` -> `11`, `2`\n",
    "\n",
    "Pour Ã©viter ce genre de phÃ©nomÃ¨ne, OpenAI utilise en rÃ©alitÃ© une variante de l'algorithme BPE qui dÃ©coupe au prÃ©alable\n",
    "le texte Ã  l'aide d'une regex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(r\"\"\" ?[^\\W\\d_]+| ?\\d{1,3}|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "\n",
    "pattern.findall(\n",
    "    \"ðŸ‘‹ ä½ å¥½   Bonjour _xXxFortiche123. Vous avez : 123456 â‚¬ sur votre compte.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI utilise des regex plus compliquÃ©s pour ces tokenizers :\n",
    "\n",
    "```python\n",
    "# the main GPT text split patterns, see\n",
    "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut rÃ©Ã©crire les fonctions `count_pairs` et `train_bpe_tokenizer` pour les faire fonctionner sur des sÃ©quences de tokens :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_pair_sequences(tokens_sequences: list[list[int]]):\n",
    "    \"\"\"RÃ©cupÃ¨re la paire la plus frÃ©quente parmi une liste de liste de tokens.\"\"\"\n",
    "\n",
    "\n",
    "def train_bpe_tokenizer_sequences(\n",
    "    text: str, vocab_size: int, pattern: re.Pattern = pattern\n",
    "):\n",
    "    \"\"\"Entrainement d'un tokenizer BPE avec une regex.\"\"\"\n",
    "\n",
    "\n",
    "merges = train_bpe_tokenizer_sequences(text, 266)\n",
    "merges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
