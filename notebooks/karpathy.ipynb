{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoriel Andrej Karpathy - révision\n",
    "\n",
    "[![Youtube video cover](assets/img/yt_tokenizer_cover.webp)](https://www.youtube.com/watch?v=zduSFxRajkE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/musiques/nekfeu/niquelesclones.txt\", \"r\") as f:\n",
    "    texte = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Paroles de \"Nique les clones Part. II\"]\n",
      "\n",
      "[Intro]\n",
      "Je ne vois plus que des clones, ça a commencé à l'\n"
     ]
    }
   ],
   "source": [
    "sample = texte[:100]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tokens = list(sample.encode(\"utf-8\"))\n",
    "len(sample_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((115, 32), 6)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_pairs(tokens: list):\n",
    "    return Counter(zip(tokens, tokens[1:]))\n",
    "\n",
    "counts = count_pairs(sample_tokens)\n",
    "counts.most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge(tokens: list, pair: tuple[int, int], ind: int):\n",
    "    compressed_tokens = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < (len(tokens) - 1) and (tokens[i], tokens[i + 1]) == pair:\n",
    "            compressed_tokens.append(ind)\n",
    "            i += 2\n",
    "        else:\n",
    "            compressed_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "\n",
    "    return compressed_tokens\n",
    "\n",
    "\n",
    "merge([1, 2, 3], (1, 3), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self):\n",
    "        self.merges: dict[tuple[int, int], int] = {}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return 256 + len(self.merges)\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        try:\n",
    "            if len(self.__vocab) != self.vocab_size:\n",
    "                return self.__vocab\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        self.__vocab = {i: bytes([i]) for i in range(256)}\n",
    "        for (i1, i2), i in self.merges.items():\n",
    "            self.__vocab[i] = bytes(self.vocab[i1] + self.vocab[i2])\n",
    "\n",
    "        return self.__vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def get_top_pair(tokens: list[int]):\n",
    "        if len(tokens) < 2:\n",
    "            return None\n",
    "\n",
    "        counts = Counter(zip(tokens, tokens[1:]))\n",
    "\n",
    "        pair, _ = counts.most_common(1)[0]\n",
    "        return pair\n",
    "\n",
    "    @staticmethod\n",
    "    def merge(tokens: list[int], pair: tuple[int, int], ind: int):\n",
    "        compressed_tokens = []\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < (len(tokens) - 1) and (tokens[i], tokens[i + 1]) == pair:\n",
    "                compressed_tokens.append(ind)\n",
    "                i += 2\n",
    "            else:\n",
    "                compressed_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "\n",
    "        return compressed_tokens\n",
    "\n",
    "    def train(self, text: str, vocab_size: int):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "        while self.vocab_size < vocab_size:\n",
    "            pair = self.get_top_pair(tokens)\n",
    "\n",
    "            if pair is None:\n",
    "                break\n",
    "\n",
    "            tokens = self.merge(tokens, pair, self.vocab_size)\n",
    "            self.merges[pair] = self.vocab_size\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "        while len(tokens) >= 2:\n",
    "            pair = min(\n",
    "                zip(tokens, tokens[1:]), key=lambda p: self.merges.get(p, float(\"inf\"))\n",
    "            )\n",
    "\n",
    "            if pair in self.merges:\n",
    "                tokens = self.merge(tokens, pair, self.merges[pair])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: list[int]):\n",
    "        btext = b\"\".join([self.vocab[i] for i in tokens])\n",
    "        return btext.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.train(texte, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 260, 326, 268, 300, 109, 321, 356, 272, 118, 272, 63, 32]\n",
      "Bonjour comment ça va ? \n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"Bonjour comment ça va ? \")\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/hakim/projets-wsl/notebooks/karpathy/tokenizer2.ipynb Cellule 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hakim/projets-wsl/notebooks/karpathy/tokenizer2.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hakim/projets-wsl/notebooks/karpathy/tokenizer2.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m random\u001b[39m.\u001b[39;49mchoice({\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m})\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/3.11.7/lib/python3.11/random.py:374\u001b[0m, in \u001b[0;36mRandom.choice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(seq):\n\u001b[1;32m    373\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCannot choose from an empty sequence\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 374\u001b[0m \u001b[39mreturn\u001b[39;00m seq[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_randbelow(\u001b[39mlen\u001b[39;49m(seq))]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice({1, 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c', 'c', ' ', 'd', 'c', ' ', 'c', 'b', 'c', 'b']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterable, Hashable\n",
    "from itertools import chain, islice\n",
    "from collections import Counter\n",
    "from random import Random\n",
    "\n",
    "\n",
    "class MarkovLM:\n",
    "    def __init__(self, C: int, seed: int | None = None):\n",
    "        self.C = C\n",
    "        self.stats = {}\n",
    "        self.vocab: list = []\n",
    "\n",
    "        self._seed = seed\n",
    "        self._random = Random(seed)\n",
    "\n",
    "    def to_state_seq(self, seq: Iterable[Hashable]):\n",
    "        offsets = []\n",
    "        for i in range(self.C):\n",
    "            offsets.append(chain.from_iterable([[None] * (self.C - i), seq]))\n",
    "\n",
    "        return zip(zip(*offsets), seq)\n",
    "\n",
    "    def fit(self, seq: Iterable[Hashable]):\n",
    "        state_seq = self.to_state_seq(seq)\n",
    "\n",
    "        self.stats = Counter(state_seq)\n",
    "        self.vocab = list(set(seq).union(self.vocab))\n",
    "\n",
    "    def generate(self, seq: list[Hashable], max_tokens: int):\n",
    "        new_seq = []\n",
    "\n",
    "        if len(seq) < self.C:\n",
    "            current_state = tuple([*[None] * (self.C - len(seq)), *seq[-self.C :]])\n",
    "        else:\n",
    "            current_state = tuple(seq[-self.C :])\n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "\n",
    "            probs = {}\n",
    "            for (state, t), count in self.stats.items():\n",
    "                if state == current_state:\n",
    "                    probs[t] = probs.get(t, 0) + count\n",
    "\n",
    "            if probs:\n",
    "                probs_keys = list(probs.keys())\n",
    "                probs_weights = list(probs.values())\n",
    "\n",
    "                next_token = self._random.choices(probs_keys, weights=probs_weights)[0]\n",
    "            else:\n",
    "                next_token = self._random.choices(self.vocab)[0]\n",
    "\n",
    "            new_seq.append(next_token)\n",
    "            current_state = current_state[1:] + (next_token,)\n",
    "\n",
    "        return new_seq\n",
    "\n",
    "\n",
    "lm = MarkovLM(3)\n",
    "lm.fit(\"   ab ac ad\")\n",
    "\n",
    "lm.generate(\"a\", 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Total of weights must be greater than zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/hakim/projets-wsl/notebooks/karpathy/tokenizer2.ipynb Cellule 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/hakim/projets-wsl/notebooks/karpathy/tokenizer2.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m random\u001b[39m.\u001b[39;49mchoices([\u001b[39m\"\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m], weights\u001b[39m=\u001b[39;49m[\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/3.11.7/lib/python3.11/random.py:509\u001b[0m, in \u001b[0;36mRandom.choices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    507\u001b[0m total \u001b[39m=\u001b[39m cum_weights[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m0.0\u001b[39m   \u001b[39m# convert to float\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m total \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m--> 509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mTotal of weights must be greater than zero\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    510\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _isfinite(total):\n\u001b[1;32m    511\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mTotal of weights must be finite\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Total of weights must be greater than zero"
     ]
    }
   ],
   "source": [
    "random.choices([\"a\", \"b\"], weights=[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "x = defaultdict(Counter)\n",
    "x[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', ' ', ' ', 'a', 'b', ' ', 'a', 'c', ' ', 'a']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Hashable\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from random import Random\n",
    "\n",
    "\n",
    "class MarkovLM2:\n",
    "    def __init__(self, C: int, vocab: str | list[Hashable], seed: int | None = None):\n",
    "        self.C = C\n",
    "        self.stats: dict[tuple, dict] = defaultdict(Counter)\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self._seed = seed\n",
    "        self._random = Random(seed)\n",
    "\n",
    "    def to_state_seq(self, seq: str | list[Hashable]):\n",
    "        offsets = []\n",
    "        for i in range(self.C):\n",
    "            offsets.append(chain.from_iterable([[None] * (self.C - i), seq]))\n",
    "\n",
    "        return zip(zip(*offsets), seq)\n",
    "\n",
    "    def fit(self, seq: str | list[Hashable]):\n",
    "        for state, next_token in self.to_state_seq(seq):\n",
    "            self.stats[state][next_token] += 1\n",
    "\n",
    "    def generate(self, seq: str | list[Hashable], max_tokens: int):\n",
    "        new_seq = []\n",
    "\n",
    "        if len(seq) < self.C:\n",
    "            current_state = tuple([*[None] * (self.C - len(seq)), *seq[-self.C :]])\n",
    "        else:\n",
    "            current_state = tuple(seq[-self.C :])\n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "            if current_state in self.stats:\n",
    "                counts = self.stats[current_state]\n",
    "                next_token = self._random.choices(\n",
    "                    list(counts.keys()), weights=list(counts.values()), k=1\n",
    "                )[0]\n",
    "\n",
    "            else:\n",
    "                next_token = self._random.choice(self.vocab)\n",
    "\n",
    "            new_seq.append(next_token)\n",
    "            current_state = current_state[1:] + (next_token,)\n",
    "\n",
    "        return new_seq\n",
    "\n",
    "\n",
    "lm = MarkovLM2(10, \" abcd\")\n",
    "lm.fit(\"   ab ac ad\")\n",
    "lm.generate(\"\", 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
