---
marp: true
theme: default
lang: fr
paginate: true
header: "![h:40](images/logo_letters.png)"
footer: "Primptemps de la Tech | Comprendre les tokens par la pratique"
transition: slide
---

<style>
/* 
Couleurs : 
    - #69bfbc
    - #e12967
    - #6d6cae
    - #f3956e
    - #fbbb2b
    - #222d4e
*/
:root p {
    font-size: 1rem;
}
:root h1 {
    color: #e12967;
    font-size: 1.6rem;
}
:root h2 {
    color: #e12967;
    font-size: 1.5rem;
}
:root h3 {
    color: #e12967;
    font-size: 1.4rem;
}
:root img {
    background-color: none;
}
:root.lead {
    text-align: center;
    background: url(images/fond.png) no-repeat right bottom, linear-gradient(0deg, #ead283, #91c4aa, #6abfbc);
}

:root.lead h1,:root.lead h2,:root.lead h3 {
    color: white;
    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);
    font-size: 2rem;
}

:root.section-title {
    text-align: center;
    background: url(images/fond.png) no-repeat right bottom, linear-gradient(45deg, #F5D07A , #F5B784 );
}
:root.section-title h1 {
    color: white;
    font-size: 2.8rem;
    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);
}
</style>

<!-- Slide de pr√©sentation -->
<!-- _paginate: skip -->
<!-- _class: lead -->
<!-- _header: "" -->
<!-- _footer: "" -->

<p align="center">
    <img alt="Logo Printemps de la Tech" height="400" src="images/logo_colore.png" style="background:none;" />
</p>

# Comprendre les tokens par la pratique

---

<!-- _paginate: skip -->
<!-- _header: "" -->

![bg contain left:30%](images/profil_pdt.png)

## Hakim Cheikh
**Data Scientist chez Valeuriad depuis 3 ans**

_En mission chez France Travail_

---

## Pr√©-requis

:snake: Python niveau A1

## D√©roul√©

* Notions de traitement du langage
* Impl√©mentation d'un tokenizer
* R√©fl√©xions et conclusion

---

<!-- _paginate: skip -->
<!-- _class: section-title -->
<!-- _footer: "" -->

# Notions de traitement du langage

---

### _Quelques notions de NLP_

**NLP** : **N**atural **L**angage **P**rocessing

Exemples de cas d'usage : 

* Cat√©gorisation de contenu
* Extraction d'entit√©s 
* Analyse de sentiments
* Traduction
* R√©sum√©
* G√©n√©ration de texte

---

### _Quelques notions de NLP_ : D√©composition en mots

Les algorithmes fonctionnent √† partir d'une repr√©sentation sous forme num√©rique.

```python
text = "Bienvenue au Printemps de la Tech 2025"
```

<div data-marpit-fragment>

**Premi√®re approche :** d√©composer le texte sur les espaces puis construire un vocabulaire 


```python
words = text.split(" ")
# ['Bienvenue', 'au', 'Printemps', 'de', 'la', 'Tech', '2025']

vocab = {word: i for i, word in enumerate(words)}

text_ids = [vocab[word] for word in words]
text_ids
# [0, 1, 2, 3, 4, 5, 6]
```

</div>

---

### _Quelques notions de NLP_ : D√©composition en mots

```python
text
# "Bienvenue au Printemps de la Tech 2025"

text_ids
# [0, 1, 2, 3, 4, 5, 6]

vocab
# {'Bienvenue': 0, 'au': 1, 'Printemps': 2, 'de': 3, 'la': 4, 'Tech': 5, '2025': 6}
```

Ce que l'on vient de faire est une **tokenization** : il s'agit de repr√©senter le texte par des identifiants, les **tokens**.

---

### _Quelques notions de NLP_ : Limites de la d√©composition en mots

* ‚ö† _Out-of-vocabulary_ : Les mots inconnus sont remplac√©s par `<Unknown>`.
* ‚ö† _Racine des mots_ : Identifiants diff√©rents pour les mots de m√™me racine (mobile / automobile / immobile, ...)
* ‚úÖ _Taille des s√©quences_ : Raisonnable, un token par mot.
* ‚ö† _Taille du vocabulaire_ : Grand vocabulaire pour g√©rer le multilingue.

---

### _Quelques notions de NLP_ : D√©composition en caract√®res

```python
text = "Bienvenue au Printemps de la Tech 2025"
```

<div data-marpit-fragment>

**Seconde approche :** d√©composer le texte en caract√®res

```python
text_ids = [ord(c) for c in text]
text_ids
#   B,   i,   e,   n,   v,   e,   n,   u,  e,   ¬∑,  a,   u,  ¬∑,  P,   r,   i,   n,   t,
# [66, 105, 101, 110, 118, 101, 110, 117, 101, 32, 97, 117, 32, 80, 114, 105, 110, 116, ...]
```

Le vocabulaire est l'ensemble des caract√®res unicode.
</div>

---

### _Quelques notions de NLP_ : D√©compositions en caract√®res

* ‚úÖ _Out-of-vocabulary_ : Plus de probl√®me.
* ‚úÖ _Racine des mots_ : Plus de probl√®me.
* ‚ö† _Taille des s√©quences_ : Tr√®s importante, un token par caract√®re.
* ‚ùì _Taille du vocabulaire_ : √† votre avis ?

---

**Quizz** : Combien y a-t-il de points de code [unicode](https://www.unicode.org/versions/Unicode15.0.0/) ?


<div data-marpit-fragment>

**149 186 !** 
_(dans la version 15.0)_

</div>


<!-- SLIDE RETIREE PAR MANQUE DE TEMPS

---

### _Quelques notions de NLP_ : N-grams

Une troisi√®me approche est de d√©composer le texte en **n-grams** :

```python
text = "Bienvenue au Printemps de la Tech 2025"

N = 3
tokens = [text[i:i+N] for i in range(0, len(text))]
tokens
# ['Bie', 'ien', 'env', 'nve', 'ven', 'enu', 'nue', 'ue ', 'e a', ' au', 'au ', 'u P', ...]
```

* L'ordre devient moins important
* Mais la taille du vocabulaire explose

-->

---

### Tokenization "moderne"

![bg contain right:60%](images/screen_tiktokenizer.png)

Aujourd'hui OpenAI utilise l'algorithme _Byte Pair Encoding_ introduit par Philip Gage en 1994 pour ChatGPT.

[D√©mo](https://tiktokenizer.vercel.app/)


---

<!-- _paginate: skip -->
<!-- _class: section-title -->
<!-- _footer: "" -->

# Construison un Tokenizer

---

### Caract√®res unicode

Un texte est une s√©quence de caract√®res unicode (appel√©s points de code).

Les points de codes sont ensuites encod√©s en octets (bytes) via un encodage.

Ci-dessous l'encodage `UTF-8` de `Bonjour üëã`

![Encodage utf-8 de bonjour](images/encodage.svg)

---

L'id√©e de l'algorithme BPE est de construire des tokens en fusionnant deux √† deux des octets de l'encodage `UTF-8`.


---

<div data-marpit-fragment>

Supposons que l'on ait la s√©quence suivante : 

```
aaabdaaabac
```

</div>
<div data-marpit-fragment>


La paire la plus fr√©quente est `aa`, on la remplace par un nouveau token `Z` :

```
ZabdZabac
Z = aa
```

</div>
<div data-marpit-fragment>


La nouvelle paire la plus fr√©quente est `ab` que l'on remplace par `Y` :

```
ZYdZYac
Y=ab
Z=aa
```

</div>

---

```
ZYdZYac
Y=ab
Z=aa
```

<div data-marpit-fragment>

Maintenant, la paire la plus fr√©quente est `ZY` que l'on remplace par `X` :

```
XdXac
X=ZY
Y=ab
Z=aa
```

</div>
<div data-marpit-fragment>

Il y a d√©sormais 7 tokens : `a, b, c, d, Z, Y, X` et l'encodage de `aaabdaaabac` est `XdXac`

</div>

---

### R√©sum√© de l'algorithme BPE

* On d√©marre avec un token par octet : de `0` √† `255`
* On construit ensuite des nouveaux tokens en fusionnant la paire la plus fr√©quente : 
  * On d√©termine la paire la plus fr√©quente
  * On lui associe un nouvel indice : `256`, puis `257`, etc.
* Avantages : 
  * ‚úÖ _Out-of-vocabulary_ : Plus de probl√®me.
  * üÜí _Racine des mots_ : Les caract√®res qui apparaissent souvent sont fusionn√©es ensembles.
  * ‚úÖ _Taille des s√©quences_ : Maitris√©e par la taille de vocabulaire.
  * ‚úÖ _Taille du vocabulaire_ : Fix√©e √† l'avance.

---

### Etape 1 : Trouver la paire la plus fr√©quente

Ecrire une fonction `get_top_pair` qui prend en entr√©e une liste d'entiers (liste de tokens) et qui renvoie la paire la plus fr√©quente (ou `None` dans le cas o√π il n'y a aucune paire).

```python
def get_top_pair(ids: list[int]) -> tuple[tuple[int, int], int] | None:
    """R√©cup√©ration de la paire de tokens la plus fr√©quente."""
```

Exemple : 

```python
get_top_pair([1, 2, 1, 2])

# ((1, 2), 2)
```

---

### Etape 2 : Fusionner la paire la plus fr√©quente

Ecrire une fonction `merge` qui prend en entr√©e une liste de tokens, une paire et un id de nouveau token et qui remplace les occurences de la paire par le nouvel id.

```python
def merge(ids: list[int], pair: tuple[int, int], new_id: int) -> list[int]:
    """Fusion d'une pair de tokens."""
```

Exemple : 

```python
merge([1, 2, 3, 1, 2, 3, 3], (2, 3), 4)

# [1, 4, 1, 4, 3]
```

---
<style scoped>
section {
    font-size: 1.6rem;
}
</style>


### Etape 3 : Entrainement d'un tokenizer BPE

Ecrire une fonction `train_bpe_tokenizer` qui prend en entr√©e un texte et une taille de vocabulaire et qui renvoie en sortie un dictionnaire contenant les paires fusionn√©es et leurs ids.

```python
def train_bpe_tokenizer(text: str, vocab_size: int) -> dict[tuple[int, int], int]:
    """Entrainement d'un tokenizer BPE."""
```

Exemple : 

```python
merges = train_bpe_tokenizer("bonjour", 266)

# {
#   (98, 111): 256,
#   (256, 110): 257,
#   (257, 106): 258,
#   (258, 111): 259,
#   (259, 117): 260,
#   (260, 114): 261,
# }
```

---

### Etape 4 : Fonction d'encodage (tokenization)

Ecrire une fonction `encode` qui prend en entr√©e un texte et un dictionnaire de fusions et qui renvoie la liste des tokens.

```python
def encode(text: str, merges: dict[tuple[int, int], int]) -> list[int]:
    """Tokenization d'une chaine de caract√®re √† partir du dictionnaire merges."""
```

Exemple : 

```python
tokens = encode(
    "ploc, ploc",
    {(112, 108): 256, (32, 256): 257, (111, 99): 258, (258, 44): 259},
)

# [256, 259, 257, 258]
```

---
<style scoped>
section {
    font-size: 1.3rem;
}
</style>

### Etape 5 : Construction du vocabulaire

Ecrire une fonction `construct_vocab` qui prend en entr√©e un dictionnaire de fusions et qui renvoie un dictionnaire qui √† chaque id de token associe une s√©quences de `bytes`.

```python
def construct_vocab(merges: dict[tuple[int, int], int]) -> dict[int, bytes]:
    """Construction d'un vocabulaire √† partir d'un dictionnaire merges."""
```

Le vocabulaire de base, ne contenant que les tokens initiaux de 0 √† 255, peut √™tre construit de la mani√®re suivantes : 

```python
BASE_VOCAB = {idx: bytes([idx]) for idx in range(256)}
```

Exemple : 

```python
construct_vocab({(112, 108): 256, (32, 256): 257, (111, 99): 258})

# {0: b'\x00',
#  ...,
#  255: b'\xff',
#  256: b'pl',
#  257: b' pl',
#  258: b'oc'}
```

---

### Etape 6 : D√©codage

Ecrire une fonction `decode` qui prend en entr√©e une liste de tokens et renvoie en sortie le texte correspondant.

```python
def decode(tokens: list[int], vocab: dict[int, bytes]) -> str:
    """D√©codage d'une s√©quence de tokens"""
```

Exemple : 

```python
BASE_VOCAB = {idx: bytes([idx]) for idx in range(256)}

decode(
    [256, 259, 257, 258],
    {**BASE_VOCAB, 256: b"pl", 257: b" pl", 258: b"oc", 259: b"oc,"}
)

# 'ploc, ploc'
```

---
<style scoped>
section {
    font-size: 1.6rem;
}
</style>

## Bonus : Tokenization BPE et regex !

Une des limites de l'algorithme BPE est que les mots courants tels que `chien` peuvent tout √† faire se retrouver dans plusieurs tokens avec une ponctuation diff√©rente :  `chien`, ` chien`,`chien.`, `chien,`, `chien!`, `chien?`.

<div data-marpit-fragment>

Cela est probl√©matique pour les mod√®les de langage car le mod√®le doit "comprendre" √† partir des textes sur lesquels il est entraint√© que tous ces tokens d√©signe le m√™me concept.

</div>
<div data-marpit-fragment>

C'est √©galement un probl√®me pour les nombres car il peut tr√®s bien y avoir uniquement des tokens `1`, `11`, `2` ce qui provoque des tokenization √©trange des nombres : 
* `123` -> `1`, `2`, `3`
* `112` -> `11`, `2`

</div>
<div data-marpit-fragment>

Pour √©viter ce genre de ph√©nom√®ne, OpenAI utilise en r√©alit√© une variante de l'algorithme BPE qui d√©coupe au pr√©alable le texte √† l'aide d'une regex.

</div>

---

Exemple de la regex du tokenizer de GPT-2 d'OpenAI :

```python
# Pattern GPT-2

r"'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"

```

On r√©cup√®re uniquement les √©l√©ments du texte qui match ce pattern puis on calcule la paire la plus fr√©quente sur ces √©l√©ments. Cela √©vite la fusion de certains tokens qui ne pourront jamais appara√Ætre √† la suite.


---

### Token sp√©ciaux

![bg right:70%](images/screen_tiktokenizer_spe.png)

Pour d√©limiter les tours de conversation, la fin de texte, etc. il y a aussi des tokens sp√©ciaux qui sont g√©r√©s √† part.

[D√©mo](https://tiktokenizer.vercel.app/)

---

<!-- _paginate: skip -->
<!-- _class: section-title -->
<!-- _footer: "" -->

# Limites, r√©flexions et conclusion

---

### Capacit√© √† raisonner sur les mots

![llms qui n'arrivent pas √† compter les r dans erreur](images/r_erreur.png)

---

### Capacit√©s arithm√©tiques

Les performances des mod√®les notamment en arithm√©tique sont d√©pendantes de la tokenization.

_[Integer tokenization is insane](https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/)_

_[Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs](https://arxiv.org/html/2402.14903v1)_

![bg contain right:50%](images/arithmetic_tokenization.png)

---

### Fronti√®re des tokens

Compl√©tons le texte : `le gar√ßon joue √†`
* on obtiens les tokens : `le` ` gar√ßon` ` joue` ` √†`
* le LLM pr√©dit la suite des tokens : `[282, 122357, 74342, 1221]` 
* `[282, 122357, 74342, 1221,`  `557, 147386]`
* `le gar√ßon joue √† la balle`

A pr√©sent, compl√©tons le texte : `le gar√ßon joue √† `
* on obtiens les tokens :`le` ` gar√ßon` ` joue` ` √†` ` `
* le LLM pr√©dit la suite des tokens : `[282, 122357, 74342, 1221, 220]` 
* `[282, 122357, 74342, 1221, 220, ` `1675, 147386]`
* `le gar√ßon joue √† la balle`

---

### Fronti√®re des tokens : Token healing

Pour √©viter les probl√®mes li√©s au fronti√®res des tokens, on peut utiliser la technique du _token healing_.

L'id√©e est de supprimer le dernier token et forcer le premier token g√©n√©r√© √† matcher le d√©but du token supprim√©.

_[The Art of Prompt Design: Prompt Boundaries and Token Healing](https://medium.com/data-science/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38)_


---

### Performances

La tokenization a un impact sur les performances des mod√®les de langage.

L'utilisation d'un tokenizer entrain√© sur des textes anglais provoque :
* une baisse de performance des LLM
* une augmentation des co√ªts d'inf√©rence (jusqu'√† 68% dans le papier [1])

<div data-marpit-fragment>

[1] _[Tokenizer Choice For LLM Training: Negligible or Crucial?](https://arxiv.org/html/2310.08754v4)_

</div>

---

### Co√ªt


![h:400](images/nb_tokens_langs.png)

_[Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models](https://arxiv.org/pdf/2305.13707)_

![bg contain right:30%](images/cost.png)

---

### Un futur sans tokenization ?

Face aux limites de la tokenization, des chercheurs essayent de s'en passer compl√®tement :

_[MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/pdf/2305.07185)_

Pour le moment tous les LLMs leader reposent sur la tokenization.

---

<!-- _paginate: skip -->
<!-- _class: section-title -->
<!-- _footer: "" -->

# Merci pour votre attention

## Des questions ou remarques ? 

---

## Merci √† Andrej Karpathy

Cet atelier est bas√© sur sa vid√©o [Let's build the GPT Tokenizer ](https://www.youtube.com/watch?v=zduSFxRajkE)

![bg right:60% contain](images/andrej_tokenization.jpg)

---
<style scoped>
section {
    font-size: 1.2rem;
}
</style>

## R√©f√©rences

- _[Let's build the GPT Tokenizer ](https://www.youtube.com/watch?v=zduSFxRajkE)_
- _[Standard unicode](https://www.unicode.org/versions/Unicode15.0.0/)_
- _[Tiktokenizer](https://tiktokenizer.vercel.app/)_
- _[Integer tokenization is insane](https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/)_
- _[Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs](https://arxiv.org/html/2402.14903v1)_
- _[The Art of Prompt Design: Prompt Boundaries and Token Healing](https://medium.com/data-science/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38)_
- _[Tokenizer Choice For LLM Training: Negligible or Crucial?](https://arxiv.org/html/2310.08754v4)_
- _[Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models](https://arxiv.org/pdf/2305.13707)_
- _[MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/pdf/2305.07185)_
- _[Integer tokenization is insane](https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/)_
- _[Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs](https://arxiv.org/html/2402.14903v1)_
- _[The Art of Prompt Design: Prompt Boundaries and Token Healing](https://medium.com/data-science/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38)_
- _[Tokenizer Choice For LLM Training: Negligible or Crucial?](https://arxiv.org/html/2310.08754v4)_
- _[Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models](https://arxiv.org/pdf/2305.13707)_
- _[MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/pdf/2305.07185)_