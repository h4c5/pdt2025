---
marp: true
theme: default
lang: fr
paginate: true
header: "![h:60](images/logo_letters.png)"
footer: "Primptemps de la Tech | Comprendre les tokens par la pratique"
---

<style>
/* 
Couleurs : 
    - #69bfbc
    - #e12967
    - #6d6cae
    - #f3956e
    - #fbbb2b
    - #222d4e
*/

:root.lead {
    text-align: center;
}

:root.lead h1,h2,h3 {
    color: white;
}

:root h1,h2,h3 {
    color: #e12967;
}

</style>

<!-- Slide de pr√©sentation -->
<!-- _paginate: skip -->
<!-- _class: lead -->
<!-- _header: "" -->
<!-- _footer: "" -->
<!-- _backgroundImage: "linear-gradient(0deg, #ead283, #91c4aa, #6abfbc)"  -->

![Logo Printemps de la Tech h:400](images/logo_colore.png)

# Comprendre les tokens par la pratique

---

<!-- _paginate: skip -->
<!-- _header: "" -->

![bg contain left:30%](images/profil_pdt.png)

## Hakim Cheikh
**Data Scientist chez Valeuriad depuis 3 ans**

_En mission chez France Travail_

---

## Pr√©-requis

:snake: Python niveau A1

## D√©roul√©

1. Introduction √† la tokenization
2. Impl√©mentation d'un tokenizer
3. Remarques et conclusion

---

### _Quelques notions de NLP_

**NLP** : **N**atural **L**angage **P**rocessing

Exemples de cas d'usage : 

- Cat√©gorisation de contenu
- Extraction d'entit√©s 
- Analyse de sentiments
- Traduction
- R√©sum√©
- G√©n√©ration de texte

---

### _Quelques notions de NLP_ : D√©compositions en mots

Les algorithmes fonctionnent √† partir d'une repr√©sentation sous forme num√©rique.

```python
text = "Bienvenu au Printemps de la Tech 2025"
```

**Premi√®re approche :** d√©composer le texte sur les espaces puis construire un vocabulaire

```python
words = text.split(" ")
# ['Bienvenu', 'au', 'Printemps', 'de', 'la', 'Tech', '2025']

vocab = {word: i for i, word in enumerate(words)}

text_ids = [vocab[word] for word in words]
text_ids
# [0, 1, 2, 3, 4, 5, 6]
```

---

### _Quelques notions de NLP_ : D√©compositions en mots

```python
text
# "Bienvenu au Printemps de la Tech 2025"

text_ids
# [0, 1, 2, 3, 4, 5, 6]

vocab
# {'Bienvenu': 0, 'au': 1, 'Printemps': 2, 'de': 3, 'la': 4, 'Tech': 5, '2025': 6}
```

Ce que l'on vient de faire est une **tokenization** : il s'agit de repr√©senter le texte en **tokens**, c'est-√†-dire en unit√©s √©l√©mentaires.

---

### _Quelques notions de NLP_ : D√©compositions en mots

- Le vocabulaire est construit √† partir de textes de r√©f√©rence
- Les mots qui ne se trouvent pas dans les textes de r√©f√©rences ne pourront pas √™tre interpr√©t√©s de la m√™me mani√®re
- Les mots qui ont la m√™me racine ont des identifiants compl√®tement diff√©rents
  - Exemple : mobile / automobile / mobilit√© / immobile, immobilier, etc.
- Pour adresser les points pr√©c√©dents, il existe des techniques de pr√©-traitement (mise en minuscule, lemmatization, √©limination des mots trop courants, etc.)
- Pour certains cas d'usage on peut ignorer l'ordre des mots.

---

### _Quelques notions de NLP_ : D√©compositions en caract√®res

```python
text = "Bienvenu au Printemps de la Tech 2025"
```

**Seconde approche :** d√©composer le texte en caract√®res

```python
text_ids = [ord(c) for c in text]
text_ids
#   B,   i,   e,   n,   v,   e,   n,   u,  ¬∑,  a,   u,  ¬∑,  P,   r,   i,   n,   t,
# [66, 105, 101, 110, 118, 101, 110, 117, 32, 97, 117, 32, 80, 114, 105, 110, 116, ...]
```

---

### _Quelques notions de NLP_ : D√©compositions en caract√®res

- Le vocabulaire se r√©sume aux caract√®res unicode.
- Le probl√®me des mots qui partagent la m√™me racine a disparu mais l'ordre des lettres est important.
- Un m√™me texte compte maintenant bien plus tokens.

---

**Quizz** : Combien y a-t-il de point de code [unicode](https://www.unicode.org/versions/Unicode15.0.0/) ?

---

**Quizz** : Combien y a-t-il de point de code [unicode](https://www.unicode.org/versions/Unicode15.0.0/) ?

**149 186 !** 
_(dans la version 15.0)_

---

### _Quelques notions de NLP_ : N-grams

Une troisi√®me approche est de d√©composer le texte en **n-grams** :

```python
text = "Bienvenu au Printemps de la Tech 2025"

N = 3
tokens = [text[i:i+N] for i in range(0, len(text))]
tokens
# ['Bie', 'ien', 'env', 'nve', 'ven', 'enu', 'nu ', 'u a', ' au', 'au ', 'u P', ...]
```

- L'ordre devient moins important
- Mais la taille du vocabulaire explose

---

### Tokenization "moderne"

![bg right:70%](images/screen_tiktokenizer.png)

Aujourd'hui OpenAI utilise l'algorithme _Byte Pair Encoding_ introduit par Philip Gage en 1994 pour ChatGPT.

[D√©mo](https://tiktokenizer.vercel.app/)

---

### Construison un BPE tokenizer

---

Les `str` en python sont des s√©quences imutables de point de codes unicode.

Chaque caract√®re √† un num√©ro, une cat√©gorie, un nom : 

```python
import unicodedata

text = "Bonjour üëã"

for char in text:
    print(char, ord(char), unicodedata.category(char), unicodedata.name(char))

# B    66   Lu LATIN CAPITAL LETTER B
# o   111   Ll LATIN SMALL LETTER O
# n   110   Ll LATIN SMALL LETTER N
# j   106   Ll LATIN SMALL LETTER J
# o   111   Ll LATIN SMALL LETTER O
# u   117   Ll LATIN SMALL LETTER U
# r   114   Ll LATIN SMALL LETTER R
#     32    Zs SPACE
# üëã 128075 So WAVING HAND SIGN
```

---

# Merci !

Questions & R√©ponses