---
marp: true
theme: default
lang: fr
paginate: true
header: "![h:40](images/logo_letters.png)"
footer: "Primptemps de la Tech | Comprendre les tokens par la pratique"
---

<style>
/* 
Couleurs : 
    - #69bfbc
    - #e12967
    - #6d6cae
    - #f3956e
    - #fbbb2b
    - #222d4e
*/

:root.lead {
    text-align: center;
    background: url(images/fond.png) no-repeat right bottom, linear-gradient(0deg, #ead283, #91c4aa, #6abfbc);
}

:root.lead h1,h2,h3 {
    color: white;
    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);
    font-size: 2rem;
}

:root h1,h2,h3 {
    color: #e12967;
}

:root.section-title {
    text-align: center;
    background: url(images/fond.png) no-repeat right bottom, linear-gradient(45deg, #F5D07A , #F5B784 );
}
:root.section-title h1 {
    color: white;
    font-size: 2.8rem;
    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);
}
</style>

<!-- Slide de pr√©sentation -->
<!-- _paginate: skip -->
<!-- _class: lead -->
<!-- _header: "" -->
<!-- _footer: "" -->

![Logo Printemps de la Tech h:400](images/logo_colore.png)

# Comprendre les tokens par la pratique

---

<!-- _paginate: skip -->
<!-- _header: "" -->

![bg contain left:30%](images/profil_pdt.png)

## Hakim Cheikh
**Data Scientist chez Valeuriad depuis 3 ans**

_En mission chez France Travail_

---

## Pr√©-requis

:snake: Python niveau A1

## D√©roul√©

1. Notions de traitement du langage
2. Impl√©mentation d'un tokenizer
3. R√©fl√©xions et conclusion

---

<!-- _paginate: skip -->
<!-- _class: section-title -->
<!-- _footer: "" -->

# Notions de traitement du langage

---

### _Quelques notions de NLP_

**NLP** : **N**atural **L**angage **P**rocessing

Exemples de cas d'usage : 

- Cat√©gorisation de contenu
- Extraction d'entit√©s 
- Analyse de sentiments
- Traduction
- R√©sum√©
- G√©n√©ration de texte

---

### _Quelques notions de NLP_ : D√©compositions en mots

Les algorithmes fonctionnent √† partir d'une repr√©sentation sous forme num√©rique.

```python
text = "Bienvenu au Printemps de la Tech 2025"
```

**Premi√®re approche :** d√©composer le texte sur les espaces puis construire un vocabulaire

```python
words = text.split(" ")
# ['Bienvenu', 'au', 'Printemps', 'de', 'la', 'Tech', '2025']

vocab = {word: i for i, word in enumerate(words)}

text_ids = [vocab[word] for word in words]
text_ids
# [0, 1, 2, 3, 4, 5, 6]
```

---

### _Quelques notions de NLP_ : D√©compositions en mots

```python
text
# "Bienvenu au Printemps de la Tech 2025"

text_ids
# [0, 1, 2, 3, 4, 5, 6]

vocab
# {'Bienvenu': 0, 'au': 1, 'Printemps': 2, 'de': 3, 'la': 4, 'Tech': 5, '2025': 6}
```

Ce que l'on vient de faire est une **tokenization** : il s'agit de repr√©senter le texte par des identifiants, les **tokens**.

---

### _Quelques notions de NLP_ : D√©compositions en mots

- Le vocabulaire est construit √† partir de textes de r√©f√©rence
- Les mots qui ne se trouvent pas dans les textes de r√©f√©rences ne pourront pas √™tre interpr√©t√©s de la m√™me mani√®re (identifiants manquants)
- Les mots qui ont la m√™me racine ont des identifiants compl√®tement diff√©rents
  - Exemple : mobile / automobile / mobilit√© / immobile, immobilier, etc.
- Pour adresser les points pr√©c√©dents, il existe des techniques de pr√©-traitement (mise en minuscule, lemmatization, √©limination des mots trop courants, etc.)
- Pour certains cas d'usage on peut ignorer l'ordre des mots.

---

### _Quelques notions de NLP_ : D√©compositions en caract√®res

```python
text = "Bienvenu au Printemps de la Tech 2025"
```

**Seconde approche :** d√©composer le texte en caract√®res

```python
text_ids = [ord(c) for c in text]
text_ids
#   B,   i,   e,   n,   v,   e,   n,   u,  ¬∑,  a,   u,  ¬∑,  P,   r,   i,   n,   t,
# [66, 105, 101, 110, 118, 101, 110, 117, 32, 97, 117, 32, 80, 114, 105, 110, 116, ...]
```

---

### _Quelques notions de NLP_ : D√©compositions en caract√®res

- Le vocabulaire se r√©sume aux caract√®res unicode (appel√©s __points de code__).
- Le probl√®me des mots qui partagent la m√™me racine a disparu mais l'ordre des lettres est important.
- Un m√™me texte compte maintenant bien plus tokens.

---

**Quizz** : Combien y a-t-il de points de code [unicode](https://www.unicode.org/versions/Unicode15.0.0/) ?

---

**Quizz** : Combien y a-t-il de points de code [unicode](https://www.unicode.org/versions/Unicode15.0.0/) ?

**149 186 !** 
_(dans la version 15.0)_

---

### _Quelques notions de NLP_ : N-grams

Une troisi√®me approche est de d√©composer le texte en **n-grams** :

```python
text = "Bienvenu au Printemps de la Tech 2025"

N = 3
tokens = [text[i:i+N] for i in range(0, len(text))]
tokens
# ['Bie', 'ien', 'env', 'nve', 'ven', 'enu', 'nu ', 'u a', ' au', 'au ', 'u P', ...]
```

- L'ordre devient moins important
- Mais la taille du vocabulaire explose

---

### Tokenization "moderne"

![bg right:70%](images/screen_tiktokenizer.png)

Aujourd'hui OpenAI utilise l'algorithme _Byte Pair Encoding_ introduit par Philip Gage en 1994 pour ChatGPT.

[D√©mo](https://tiktokenizer.vercel.app/)


---

<!-- _paginate: skip -->
<!-- _class: section-title -->
<!-- _footer: "" -->

# Construison un Tokenizer

---

Les `str` en python sont des s√©quences imutables de point de codes unicode.

Chaque caract√®re √† un num√©ro, une cat√©gorie, un nom : 

```python
import unicodedata

text = "Bonjour üëã"

for char in text:
    print(char, ord(char), unicodedata.category(char), unicodedata.name(char))

# B    66   Lu LATIN CAPITAL LETTER B
# o   111   Ll LATIN SMALL LETTER O
# n   110   Ll LATIN SMALL LETTER N
# j   106   Ll LATIN SMALL LETTER J
# o   111   Ll LATIN SMALL LETTER O
# u   117   Ll LATIN SMALL LETTER U
# r   114   Ll LATIN SMALL LETTER R
#     32    Zs SPACE
# üëã 128075 So WAVING HAND SIGN
```

---

Les chaines de caract√®res peuvent ensuite √™tre encod√©es pour permettre la sauvegarde, la lecture, etc.

Il existe plusieurs types d'encodage. Le plus utilis√© est `UTF-8`. Cet encodage repr√©sente chaque point de code par une suite de 1 √† 4 bytes en fonction de son indice :


| Premier point de code | Dernier code point | Byte 1       | Byte 2   | Byte 3   | Byte 4   |
| --------------------- | ------------------ | ------------ | -------- | -------- | -------- |
| U+0000 (0)            | U+007F (127)       | **0**yyyzzzz |          |          |          |
| U+0080 (128)          | U+07FF (2047)      | **110**xxxyy | 10yyzzzz |          |          |
| U+0800 (2048)         | U+FFFF (65535)     | **1110**wwww | 10xxxxyy | 10yyzzzz |          |
| U+010000 (65536)      | U+10FFFF (1114111) | **11110**uvv | 10vvwwww | 10xxxxyy | 10yyzzzz |

<br/>

> **Rappel :** Un _byte_ en anglais correspond √† un _octet_ en fran√ßais soit 8 _bits_.
> 
> Un octet peut donc prendre 2^8 = 256 valeurs

---

```python
text = "Bonjour üëã"

for char in text:
    for byte in char.encode("utf-8"):
        print(char, "|", byte, f"| code hexad√©cimal : {byte:02x}")

# B  |  66 | code hexad√©cimal : 42
# o  | 111 | code hexad√©cimal : 6f
# n  | 110 | code hexad√©cimal : 6e
# j  | 106 | code hexad√©cimal : 6a
# o  | 111 | code hexad√©cimal : 6f
# u  | 117 | code hexad√©cimal : 75
# r  | 114 | code hexad√©cimal : 72
#    |  32 | code hexad√©cimal : 20
# üëã | 240 | code hexad√©cimal : f0
# üëã | 159 | code hexad√©cimal : 9f
# üëã | 145 | code hexad√©cimal : 91
# üëã | 139 | code hexad√©cimal : 8b
```

---

L'algorithme BPE part de la liste des octets de l'encodage UTF-8 : 

```python

text = "Bonjour üëã"
text_ids = list(text.encode())

# [66, 111, 110, 106, 111, 117, 114, 32, 240, 159, 145, 139]
```

---

Ensuite, l'algorithme cr√©e des nouveaux tokens en fusionnant sucessivement la paire de tokens la plus fr√©quente.

Supposons que l'on ait la s√©quence suivante : 

```
aaabdaaabac
```

La paire la plus fr√©quente est `aa`, on la remplace par un nouveau token `Z` :

```
ZabdZabac
Z = aa
```

La nouvelle paire la plus fr√©quente est `ab` que l'on remplace par `Y` :

```
ZYdZYac
Y=ab
Z=aa
```

---

```
ZYdZYac
Y=ab
Z=aa
```

Maintenant la paire la plus fr√©quente est `ZY` que l'on remplace par `X` :

```
XdXac
X=ZY
Y=ab
Z=aa
```

Il y a d√©sormais 7 tokens : `a, b, c, d, Z, Y, X` et l'encodage de `aaabdaaabac` est `XdXac`

---

### Etape 1 : Trouver la paire la plus fr√©quente

Ecrire une fonction `get_top_pair` qui prend en entr√©e une liste d'entiers (liste de tokens) et qui renvoie la paire la plus fr√©quente (ou `None` dans le cas o√π il n'y a aucune paire).

```python
def get_top_pair(ids: list[int]) -> tuple[tuple[int, int], int] | None:
    """R√©cup√©ration de la paire de tokens la plus fr√©quente."""
```

Exemple : 

```python
get_top_pair([1, 2, 1, 2])

# ((1, 2), 2)
```

---

### Etape 2 : Fusionner la paire la plus fr√©quente

Ecrire une fonction `merge` qui prend en entr√©e une liste de tokens, une paire et un id de nouveau token et qui remplace les occurences de la paire par le nouvel id.

```python
def merge(ids: list[int], pair: tuple[int, int], new_id: int) -> list[int]:
    """Fusion d'une pair de tokens."""
```

Exemple : 

```python
merge([1, 2, 3, 1, 2, 3, 3], (2, 3), 4)

# [1, 4, 1, 4, 3]
```

---
<style scoped>
section {
    font-size: 1.6rem;
}
</style>


### Etape 3 : Entrainement d'un tokenizer BPE

Ecrire une fonction `train_bpe_tokenizer` qui prend en entr√©e un texte et une taille de vocabulaire et qui renvoie en sortie un dictionnaire contenant les paires fusionn√©es et leurs ids.

```python
def train_bpe_tokenizer(text: str, vocab_size: int) -> dict[tuple[int, int], int]:
    """Entrainement d'un tokenizer BPE."""
```

Exemple : 

```python
merges = train_bpe_tokenizer("bonjour", 266)

# {
#   (98, 111): 256,
#   (256, 110): 257,
#   (257, 106): 258,
#   (258, 111): 259,
#   (259, 117): 260,
#   (260, 114): 261,
# }
```

---

### Etape 4 : Fonction d'encodage (tokenization)

Ecrire une fonction `encode` qui prend en entr√©e un texte et un dictionnaire de fusions et qui renvoie la liste des tokens.

```python
def encode(text: str, merges: dict[tuple[int, int], int]) -> list[int]:
    """Tokenization d'une chaine de caract√®re √† partir du dictionnaire merges."""
```

Exemple : 

```python
tokens = encode(
    "ploc, ploc",
    {(112, 108): 256, (32, 256): 257, (111, 99): 258, (258, 44): 259},
)

# [256, 259, 257, 258]
```

---
<style scoped>
section {
    font-size: 1.3rem;
}
</style>

### Etape 5 : Construction du vocabulaire

Ecrire une fonction `construct_vocab` qui prend en entr√©e un dictionnaire de fusions et qui renvoie un dictionnaire qui √† chaque id de token associe une s√©quences de `bytes`.

```python
def construct_vocab(merges: dict[tuple[int, int], int]) -> dict[int, bytes]:
    """Construction d'un vocabulaire √† partir d'un dictionnaire merges."""
```

Le vocabulaire de base, ne contenant que les tokens initiaux de 0 √† 255, peut √™tre construit de la mani√®re suivantes : 

```python
BASE_VOCAB = {idx: bytes([idx]) for idx in range(256)}
```

Exemple : 

```python
construct_vocab({(112, 108): 256, (32, 256): 257, (111, 99): 258})

# {0: b'\x00',
#  ...,
#  255: b'\xff',
#  256: b'pl',
#  257: b' pl',
#  258: b'oc'}
```

---

### Etape 6 : D√©codage

Ecrire une fonction `decode` qui prend en entr√©e une liste de tokens et renvoie en sortie le texte correspondant.

```python
def decode(tokens: list[int], vocab: dict[int, bytes]) -> str:
    """D√©codage d'une s√©quence de tokens"""
```

Exemple : 

```python
BASE_VOCAB = {idx: bytes([idx]) for idx in range(256)}

decode(
    [256, 259, 257, 258],
    {**BASE_VOCAB, 256: b"pl", 257: b" pl", 258: b"oc", 259: b"oc,"}
)

# 'ploc, ploc'
```

---
<style scoped>
section {
    font-size: 1.6rem;
}
</style>

## Bonus : Tokenization BPE et regex !

Une des limites de l'algorithme BPE est que les mots courants tels que `chien` peuvent tout √† faire se retrouver dans plusieurs tokens avec une ponctuation diff√©rente :  `chien`, ` chien`,`chien.`, `chien,`, `chien!`, `chien?`.

Cela est probl√©matique pour les mod√®les de langage car le mod√®le doit "comprendre" √† partir des textes sur lesquels il est entraint√© que tous ces tokens d√©signe le m√™me concept.

C'est √©galement un probl√®me pour les nombres car il peut tr√®s bien y avoir uniquement des tokens `1`, `11`, `2` ce qui provoque des tokenization √©trange des nombres : 
- `123` -> `1`, `2`, `3`
- `112` -> `11`, `2`

Pour √©viter ce genre de ph√©nom√®ne, OpenAI utilise en r√©alit√© une variante de l'algorithme BPE qui d√©coupe au pr√©alable le texte √† l'aide d'une regex.

---

Exemple de la regex du tokenizer de GPT-2 d'OpenAI :

```python
# Pattern GPT-2

r"'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"

```

On r√©cup√®re uniquement les √©l√©ments du texte qui match ce pattern puis on calcule la paire la plus fr√©quente sur ces √©l√©ments. Cela √©vite la fusion de certains tokens qui ne pourront jamais appara√Ætre √† la suite.


---

### Token sp√©ciaux

![bg right:70%](images/screen_tiktokenizer_spe.png)

Pour d√©limiter les tours de conversation, la fin de texte, etc. il y a aussi des tokens sp√©ciaux qui sont g√©r√©s √† part.

[D√©mo](https://tiktokenizer.vercel.app/)

---

<!-- _paginate: skip -->
<!-- _class: section-title -->
<!-- _footer: "" -->

# Limites, r√©flexions et conclusion

---

### Capacit√© √† raisonner sur les mots

![](images/r_erreur.png)

---

### Capacit√©s arithm√©tiques

Les performances des mod√®les notamment en arithm√©tique sont d√©pendantes de la tokenization.

_[Integer tokenization is insane](https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/)_

_[Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs](https://arxiv.org/html/2402.14903v1)_

![bg contain right:50%](images/arithmetic_tokenization.png)

---

### Fronti√®re des tokens

Compl√©tons le texte : `le gar√ßon joue √†`
- on obtiens les tokens : `le` ` gar√ßon` ` joue` ` √†`
- le LLM pr√©dit la suite des tokens : `[282, 122357, 74342, 1221]` 
- `[282, 122357, 74342, 1221,`  `557, 147386]`
- `le gar√ßon joue √† la balle`

A pr√©sent, compl√©tons le texte : `le gar√ßon joue √† `
- on obtiens les tokens :`le` ` gar√ßon` ` joue` ` √†` ` `
- le LLM pr√©dit la suite des tokens : `[282, 122357, 74342, 1221, 220]` 
- `[282, 122357, 74342, 1221, 220, ` `1675, 147386]`
- `le gar√ßon joue √† la balle`

---

### Fronti√®re des tokens : Token healing

Pour √©viter les probl√®mes li√©s au fronti√®res des tokens, on peut utiliser la technique du _token healing_.

L'id√©e est de supprimer le dernier token et forcer le premier token g√©n√©r√© √† matcher le d√©but du token supprim√©.

_[The Art of Prompt Design: Prompt Boundaries and Token Healing](https://medium.com/data-science/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38)_


---

### Performances

La tokenization a un impact sur les performances des mod√®les de langage.

L'utilisation d'un tokenizer entrain√© sur des textes anglais provoque :
- une baisse de performance des LLM
- une augmentation des co√ªts d'inf√©rence (jusqu'√† 68% dans le papier [1])

[1] _[Tokenizer Choice For LLM Training: Negligible or Crucial?](https://arxiv.org/html/2310.08754v4)_

---

### Co√ªt


![h:400](images/nb_tokens_langs.png)

_[Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models](https://arxiv.org/pdf/2305.13707)_

![bg contain right:30%](images/cost.png)

---

### Un futur sans tokenization ?

Face aux limites de la tokenization, des chercheurs essayent de s'en passer compl√®tement :

_[MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/pdf/2305.07185)_

Pour le moment tous les LLMs leader reposent sur la tokenization.

---

<!-- _paginate: skip -->
<!-- _class: section-title -->
<!-- _footer: "" -->

# Merci pour votre attention

## Des questions ou remarques ? 